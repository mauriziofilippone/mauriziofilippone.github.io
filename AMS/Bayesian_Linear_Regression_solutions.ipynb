{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Bayesian Linear Regression - Solution Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "In this session, you'll implement Bayesian linear regression with basis functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import warnings\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "import scipy.stats\n",
    "from matplotlib import rc\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "preamble = r\"\"\"\\renewcommand{\\familydefault}{\\sfdefault}\n",
    "            \\usepackage{sansmath} \\sansmath  \\usepackage{amsmath}\"\"\"\n",
    "rc(\"font\", **{\"family\": \"sans-serif\", \"sans-serif\": \"DejaVu Sans\"})\n",
    "rc(\"text\", **{\"usetex\": False, \"latex.preamble\": preamble})\n",
    "rc(\"figure\", **{\"dpi\": 200})\n",
    "rc(\n",
    "    \"axes\",\n",
    "    **{\"spines.right\": False, \"spines.top\": False, \"xmargin\": 0, \"ymargin\": 0.05}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## 1. A Regression Dataset\n",
    "Create a simple 1D regression dataset using the `make_regression(...)` function and plot it.\n",
    "For the moment, keep the noise variance $\\sigma_\\mathrm{n}$ small.\n",
    "NB. For better reproducibility, please remember to fix the Numpy's random seed. \n",
    "For Jupyter notebooks, this needs to be done at the beginning of each cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "true_function = lambda x: 0.5 * ((x - 1) ** 2) - 3\n",
    "\n",
    "\n",
    "def make_regression(n, sn2=0.1):\n",
    "    x = np.random.uniform(-3, 3, n)\n",
    "    y = true_function(x) + np.sqrt(sn2) * np.random.randn(*x.shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "sn2 = 1.5\n",
    "x, y = make_regression(30, sn2=sn2)\n",
    "xp = np.linspace(-4, 4)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 3], dpi=250)\n",
    "ax.plot(xp, true_function(xp), \"--k\", zorder=0, label=\"True function\")\n",
    "ax.scatter(x, y, edgecolor=\"black\", linewidth=1, facecolor=\"xkcd:orange\")\n",
    "ax.set_title(\"A regression dataset\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.margins(0.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. A review on the Gaussian likelihood\n",
    "Let's start from the basics. Remember what a likelihood is.\n",
    "The **likelihood** measures the goodness of fit of a statistical model to data for given values of \n",
    "the unknown model parameters.\n",
    "It's computed from the joint probability distribution, but viewed and used as **function** \n",
    "of the parameters only, thus treating the random variables as fixed at the observed values.\n",
    "\n",
    "A Gaussian likelihood is defined as: \n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}, \\sigma_\\mathrm{n}) = \\prod_{i=1}^N p(y_i|\\mathbf{w}, {\\mathbf{x}}_i, \\sigma_\\mathrm{n}) = \\prod_{i=1}^N \\mathcal{N}(y_i|\\tilde y_i, \\sigma_\\mathrm{n})\n",
    "\\end{equation}\n",
    "\n",
    "where, for linear regression, $\\tilde y_i = \\mathbf{w}^\\top {\\mathbf{x}}_i$.\n",
    "For numerical stability, instead of using the likelihood, we will use the **log-likelihood**.\n",
    "\n",
    "\\begin{equation}\n",
    "\\log p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}, \\sigma_\\mathrm{n}) =  \\sum_{i=1}^N  \\log\\mathcal{N}(y_i|\\tilde y_i, \\sigma_\\mathrm{n})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "Write a function to compute the log-density of a normal distribution at position $x$, given $\\mu$ and $\\sigma^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def lognormal(x, mu, var):\n",
    "    # @@ COMPLETE @@ #\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "For the moment, assume that for the sample $i^{\\mathrm{th}}$, you predict $\\tilde y = 0.3$ and $\\sigma_\\mathrm{n} = 1$. \n",
    "You know that $y = 0.4$. \n",
    "Complete the following function `gaussian_loglik(...)`, then compute the (log)likelihood for this sample and show its position on the Gaussian density with a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gaussian_loglik(y, y_tilde, sn2):\n",
    "    # @@ COMPLETE @@ #\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_gaussian(mu, var, plot_log=False, **kwargs):\n",
    "    \"\"\"A simple util to plot a gaussian pdf\"\"\"\n",
    "    x = np.linspace(mu - 5 * np.sqrt(var), mu + 5 * np.sqrt(var), 200)\n",
    "    y = lognormal(x, mu, var) if plot_log else np.exp(lognormal(x, mu, var))\n",
    "    ax = kwargs.pop(\"ax\", plt.gca())\n",
    "    ax.plot(x, y, **kwargs)\n",
    "    return ax\n",
    "\n",
    "\n",
    "y_obs =   # @@ COMPLETE @@ #\n",
    "y_tilde =   # @@ COMPLETE @@ #\n",
    "sn2 =   # @@ COMPLETE @@ #\n",
    "sample_ll =  # @@ COMPLETE @@ #\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    "plot_gaussian(y_tilde, sn2, ax=ax, label=r\"$\\mathcal{N}_y(\\widetilde{y}, \\sigma_n^2)$\")\n",
    "ax.vlines(y_obs, 0, np.exp(sample_ll), color=\"k\")\n",
    "ax.vlines(\n",
    "    y_tilde, 0, np.exp(gaussian_loglik(y_tilde, y_tilde, sn2)), ls=\"--\", color=\"k\"\n",
    ")\n",
    "ax.plot(y_obs, np.exp(sample_ll), \"ok\", label=r\"Likelihood\")\n",
    "ax.text(y_tilde + 0.1, 0.02, r\"$\\widetilde{y}$\")\n",
    "ax.text(y_obs - 0.2, 0.02, r\"$y$\")\n",
    "ax.set_ylim(0)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bayesian Linear regression\n",
    "\n",
    "In this section, you'll implement Bayesian linear regression.\n",
    "Let's start by creating the **design matrix** $\\mathbf{X}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\left[ {\\begin{array}{ccccc}\n",
    "   1 & x_1^1 & \\dots & x_1^K\\\\\n",
    "   1 & x_2^1 & \\dots & x_2^K\\\\\n",
    "   \\vdots &    \\vdots & &   \\vdots \\\\\n",
    "   1 & x_N^1 & \\dots & x_N^K\\\\\n",
    "  \\end{array} } \\right]\n",
    "$$\n",
    "\n",
    "**Exercise:**\n",
    "Complete the following function `build_features(...)` to build $\\mathbf{X}$.\n",
    "This can be done in many ways. One of them is using a double list comprehension (one index for the row and one for the column), while another one is using the numpy `column_stack()` function (recommended). In any case, inspect $\\mathbf{X}$ to make sure it looks OK (show the first entries). To fit higher order polynomials, we need to add extra columns to $\\mathbf{X}$, therefore build it with $K$ as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def build_features(x, K):\n",
    "    # @@ COMPLETE @@ #\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_features(x, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the lecture notes, let's define the prior on the parameters $\\mathbf{w}$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{w} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{S}) \n",
    "\\end{equation}\n",
    "\n",
    "For sake of simplicity, assume the covariance matrix $\\mathbf{S}$ to be diagonal $\\mathbf{S} = \\sigma_\\mathrm{w}^2\\mathbf{I}$.\n",
    "Remember that the likelihood is defined as $p(\\mathbf{y}|\\mathbf{w}, \\mathbf{X}, \\sigma_\\mathrm{n}) = \\mathcal{N}(\\mathbf{y}|\\mathbf{X}\\mathbf{w}, \\sigma_\\mathrm{n}^2\\mathbf{I})$.\n",
    "In this case, the posterior is analitic and follows this form:\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y}, \\sigma_\\mathrm{n}) = \\mathcal{N}\\left(\\frac{1}{\\sigma^2_\\mathrm{n}}\\mathbf{\\Sigma}\\mathbf{X}^\\top\\mathbf{y}, \\mathbf{\\Sigma} \\right)\n",
    "\\end{equation}\n",
    "where $\\mathbf{\\Sigma}^{-1} = \\left(\\frac{1}{\\sigma^2_\\mathrm{n}}\\mathbf{X}^\\top\\mathbf{X} + \\mathbf{S}^{-1}  \\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the dimensionality of $\\mathbf{\\Sigma}$? How much does it cost to compute that inverse? Do you know which algorithm you should use to have numerically stable results? Remember that computing $\\mathbf{A}^{-1}\\mathbf{z}$ means in practice solving a linear system.\n",
    "\n",
    "**Exercise:** Complete the following function to compute the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_posterior(X, y, sw2, sn2):\n",
    "    # @@ COMPLETE @@ #\n",
    "    return w_posterior_mean, w_posterior_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Execise:** Compute the posterior over the parameters for the given regression dataset. For the moment, place $\\sigma_\\mathrm{w}^2=1$ and start with polynomial of order 1. Finally, print the posterior mean and covariance. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "sw2 = 1\n",
    "K = 1\n",
    "\n",
    "X =   # @@ COMPLETE @@ #\n",
    "\n",
    "w_posterior_mean, w_posterior_cov = # @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(w_posterior_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "print(w_posterior_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "x_ = np.linspace(-3, 3, 50)\n",
    "y_ = np.linspace(-3, 3, 50)\n",
    "X_, Y_ = np.meshgrid(x_, y_)\n",
    "pos = np.empty(X_.shape + (2,))\n",
    "pos[:, :, 0] = X_\n",
    "pos[:, :, 1] = Y_\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "ax = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "\n",
    "rv = scipy.stats.multivariate_normal(np.zeros(2), sw2 * np.eye(2))\n",
    "plot_config = dict(\n",
    "    cmap=\"viridis\", linewidth=0, antialiased=False, ccount=500, rcount=500\n",
    ")\n",
    "ax.plot_surface(X_, Y_, rv.pdf(pos), **plot_config)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_zlabel(\"\")\n",
    "ax.set_title(\"Prior\")\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2, projection=\"3d\")\n",
    "rv = scipy.stats.multivariate_normal(w_posterior_mean, w_posterior_cov)\n",
    "ax.plot_surface(X_, Y_, rv.pdf(pos), **plot_config)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"\")\n",
    "ax.set_zlabel(\"\")\n",
    "\n",
    "ax.set_title(\"Posterior\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predictions\n",
    "Now it's time to make predictions. \n",
    "All our motivation for being Bayesian was to be able to average predictions at $\\mathbf{x}_\\mathrm{new}$, for all possible $\\mathbf{w}$.\n",
    "This is possible by computing the following expectation:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{E}_{p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y}, \\sigma_\\mathrm{n})}\\mathcal{N}(\\mathbf{w}^\\top\\mathbf{x}_\\mathrm{new}, \\sigma^2_\\mathrm{n}) = \\int \\mathcal{N}(\\mathbf{w}^\\top\\mathbf{x}_\\mathrm{new}, \\sigma^2_\\mathrm{n}) p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y}, \\sigma_\\mathrm{n}) \\mathrm{d}\\mathbf{w}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Prove that \n",
    "$\\mathbf{E}_{p(\\mathbf{w}|\\mathbf{X}, \\mathbf{y}, \\sigma_\\mathrm{n})}\\mathcal{N}(\\mathbf{w}^\\top\\mathbf{x}_\\mathrm{new}, \\sigma^2_\\mathrm{n}) = \n",
    "\\mathcal{N}(\\mathbf{x}_\\mathrm{new}^\\top\\mathbf{\\mu}, \\sigma^2_\\mathrm{n} + \\mathbf{x}_\\mathrm{new}^\\top\\mathbf{\\Sigma}\\mathbf{x}_\\mathrm{new})$, where $\\mathbf{\\mu}$ and $\\mathbf{\\Sigma}$ are the posterior mean and covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "Write a function to compute the predictive distribution. \n",
    "Usually we want to do this for multiple points, hence the for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_predictive(Xt, w_mean, w_cov, sn2):\n",
    "    def _compute_predictive_single_point(xt_i, w_mean, w_cov, sn2):\n",
    "        yt_i_mean = # @@ COMPLETE @@ #\n",
    "        yt_i_var = # @@ COMPLETE @@ #\n",
    "        return yt_i_mean, yt_i_var\n",
    "\n",
    "    yt_mean, yt_var = np.zeros(len(Xt)), np.zeros(len(Xt))\n",
    "    for i, xt_i in enumerate(Xt):  # Loop on all the points\n",
    "        yt_mean[i], yt_var[i] = _compute_predictive_single_point(\n",
    "            xt_i, w_mean, w_cov, sn2\n",
    "        )\n",
    "\n",
    "    return yt_mean, yt_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Compute and plot the predictive distribution for 100 points between -4 and +4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "xt = np.linspace(-4, 4, 250)\n",
    "Xt = build_features(xt, K)\n",
    "y_mean, y_var =   # @@ COMPLETE @@ #\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    "ax.scatter(x, y, edgecolor=\"black\", linewidth=1, facecolor=\"xkcd:orange\", zorder=10)\n",
    "ax.plot(xt, y_mean, color=\"tab:blue\", lw=3)\n",
    "\n",
    "lb = y_mean - 2 * np.sqrt(y_var)\n",
    "ub = y_mean + 2 * np.sqrt(y_var)\n",
    "ax.fill_between(xt, lb, ub, color=\".80\", lw=0)\n",
    "ax.set_title(f\"Predictive distribution with order {K}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** You can also sample from the posterior and compute the function values. A simple way to do so is to sample from the posterior over the weights and evaluate the function at any inputs.\n",
    "Complete and use the utility function below: sample 30 times from the posterior and plot the corresponding functions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def sample(Xt, w_mean, w_cov, sn2, N):\n",
    "    def _sample_single(Xt, w_mean, w_cov, sn2):\n",
    "        # @@ COMPLETE @@ #\n",
    "        return y_sample\n",
    "\n",
    "    samples = np.zeros((N, len(Xt)))\n",
    "    for i in range(N):\n",
    "        samples[i] = _sample_single(Xt, w_mean, w_cov, sn2)\n",
    "    return samples\n",
    "\n",
    "\n",
    "samples = sample(Xt, w_posterior_mean, w_posterior_cov, sn2, N=30)\n",
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    "ax.scatter(x, y, edgecolor=\"black\", linewidth=1, facecolor=\"xkcd:orange\", zorder=10)\n",
    "ax.plot(xt, samples.T, \"tab:blue\", alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Try now with different polynomial order. Let's say 2, 5, 10, 15. Compute the design matrix, the posterior on $\\mathbf{w}$ and the predictive posterior. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def bayesian_linear_regression(x, y, xt, K, sn2, sw2):\n",
    "    X = build_features(x, K) \n",
    "    Xt = build_features(xt, K)  \n",
    "    w_posterior_mean, w_posterior_cov =  # @@ COMPLETE @@ #\n",
    "    y_mean, y_var =  # @@ COMPLETE @@ #\n",
    "    samples =   # @@ COMPLETE @@ #\n",
    "    return y_mean, y_var, samples\n",
    "\n",
    "\n",
    "poly_orders = [2, 5, 10, 12]\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=[10, 6], sharex=True, sharey=True)\n",
    "axs = axs.reshape(-1)\n",
    "for ax, K in zip(axs, poly_orders):\n",
    "    y_mean, y_var, samples = bayesian_linear_regression(x, y, xt, K, sn2, sw2)\n",
    "\n",
    "    lb = y_mean - 2 * np.sqrt(y_var)\n",
    "    ub = y_mean + 2 * np.sqrt(y_var)\n",
    "\n",
    "    ax.fill_between(xt, lb, ub, color=\".80\", lw=0)\n",
    "    ax.plot(xt, samples.T, \"tab:blue\", alpha=0.2)\n",
    "    ax.plot(xt, y_mean, color=\"tab:blue\", lw=3)\n",
    "    ax.scatter(x, y, edgecolor=\"black\", linewidth=1, facecolor=\"xkcd:orange\", zorder=10)\n",
    "    ax.set_ylim(-10, 20)\n",
    "    ax.set_title(\"Order %d\" % K)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "Play with the next cell and try to answer the following questions:\n",
    "1. Set n=0; what is the plot showing?\n",
    "2. Try to increase the number of points \"observed\"; what is happening to the posterior?\n",
    "3. Try to increase the polynomial order; how are the functions behaving?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import fixed, interact, interact_manual, interactive\n",
    "\n",
    "\n",
    "def animate(n, K):\n",
    "    y_mean, y_var, samples = bayesian_linear_regression(x[:n], y[:n], xt, K, sn2, sw2)\n",
    "    fig, ax = plt.subplots(figsize=[5, 3])\n",
    "    ax.scatter(\n",
    "        x[:n], y[:n], edgecolor=\"black\", linewidth=1, facecolor=\"xkcd:orange\", zorder=10\n",
    "    )\n",
    "    ax.plot(xt, samples.T, \"tab:blue\", alpha=0.3)\n",
    "    ax.set_ylim(-10, 20)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "interaction = interact(\n",
    "    animate,\n",
    "    n=widgets.IntSlider(min=0, max=len(x), step=1, value=0, continuous_update=False),\n",
    "    K=widgets.IntSlider(min=0, max=10, step=1, value=1, continuous_update=False),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate you model: the marginal likelihood\n",
    "\n",
    "There are several ways in which you can compute the goodness of your model. The first is the likelihood itself.\n",
    "\n",
    "**Question:** Compute the loglikelihood for model with order from 0 to 7 and plot it. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_likelihood(x, y, K, sn2, sw2):\n",
    "    y_mean, y_var, _ = bayesian_linear_regression(x, y, x, K, sn2, sw2)\n",
    "    # @@ COMPLETE @@ #\n",
    "    return\n",
    "\n",
    "\n",
    "Ks = np.arange(1, 8)\n",
    "mll = [evaluate_likelihood(x, y, K, sn2, sw2) for K in Ks]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    "ax.plot(Ks, mll, \"-ok\")\n",
    "ax.margins(0.05)\n",
    "ax.set_title(\"Log-Likelihood\")\n",
    "ax.set_xlabel(\"Model (e.g. polyn. order)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Try to answer: How likely is $\\mathbf{y}$ given $\\mathbf{X}$ *and* the model (‘first/second/... order polynomial’)? Is it the same likelihood as before?\n",
    "\n",
    "So far, we’ve ignored $p(\\mathbf{y}|\\mathbf{X}, \\sigma^2_\\mathrm{n})$, the normalizing constant in Bayes rule. Being a normalization constant, it has to be equal to: \n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{y}|\\mathbf{X}, \\sigma^2_\\mathrm{n}) = \\int p(\\mathbf{y}|\\mathbf{X}, \\mathbf{w}, \\sigma^2_\\mathrm{n})\n",
    "p(\\mathbf{w})\\mathrm{d}\\mathbf{w}\n",
    "\\end{equation}\n",
    "\n",
    "We’re averaging over all values of $\\mathbf{w}$ from the prior to get a value for how good the model is.\n",
    "\n",
    "**Question:** Suppose the prior is $\\mathcal{N}(\\mu_0, \\mathbf{\\Sigma}_0)$ and the likelihood $\\mathcal{N}(\\mathbf{X}\\mathbf{w}, \\sigma^2_\\mathrm{n} \\mathbf{I})$. Derive the marginal likelihood (hint: don't solve the integral -- check the rules for Gaussian conditioning and marginalization) (big hint: check the lecture notes).\n",
    "\n",
    "**Exercise:** Write a function to compute the marginal likelihood. Remember: this is a *likelihood* not a density. You should return a number not a density. For simplicity, assume $\\mu_0 = 0$ and $\\Sigma_0 = \\sigma^2_\\mathrm{w}\\mathbf{I}$. Use `scipy.stats.multivariate_normal` for computing the logpdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "def marginal_likelihood(X, y, sw2, sn2):\n",
    "    return  # @@ COMPLETE @@ #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Do the sample plot as before, but now plot the marginal likelihood. You should see a clear pattern here; comment the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_marginal_likelihood(x, y, K, sn2, sw2):\n",
    "    X = build_features(x, K)\n",
    "    return  # @@ COMPLETE @@ #\n",
    "\n",
    "\n",
    "poly_orders = range(1, 8)\n",
    "mll = [evaluate_marginal_likelihood(x, y, K, sn2, sw2) for K in poly_orders]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[5, 3])\n",
    "ax.plot(poly_orders, mll, \"-ok\")\n",
    "ax.margins(0.05)\n",
    "\n",
    "ax.set_title(\"Log-Marginal Likelihood\")\n",
    "ax.set_xlabel(\"Model (e.g. polyn. order)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
